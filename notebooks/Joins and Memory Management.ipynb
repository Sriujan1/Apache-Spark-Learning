{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072372b6-ee0f-4ee8-aa85-5bc4a68277e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark_env\n",
    "\n",
    "spark = spark_env.create_spark_session('joins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9697cc4-4dbc-46b8-9f5a-9e0f3dd70005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7c165-6a51-4d34-ba5a-0871e6821b50",
   "metadata": {},
   "source": [
    "- In Spark, joins are not just about matching keys — they’re executed using different physical strategies under the hood based on data size, memory, and join type.\n",
    "- The most efficient join is the **Broadcast Hash Join**, which works by broadcasting the smaller DataFrame to all executor nodes. This avoids shuffling altogether and allows each executor to perform the join locally. It’s ideal when one side of the join is small enough to fit into memory (typically under 10MB by default, configurable via spark.sql.autoBroadcastJoinThreshold). You can also explicitly use it with broadcast(df_small) when joining with a large DataFrame.\n",
    "- When the data is too large to broadcast but still one side is significantly smaller, Spark may use a **Shuffle Hash Join**. This strategy involves shuffling both datasets based on the join key, and then building a hash table on one side for lookup. It's faster than Sort-Merge Join when memory is sufficient and the key distribution isn’t skewed. However, it's more sensitive to memory pressure and hash collisions and can result in spilling to disk.\n",
    "- For large-scale joins where both DataFrames are big and neither can be broadcasted, Spark uses the **Sort-Merge Join**. This strategy sorts both sides of the data on the join key and then merges them. Though it involves full shuffling and sorting, it’s the most stable and scalable approach for large joins. It’s the default choice for equi-joins when data size exceeds the broadcast threshold and there's no major skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55150cf3-09b4-41ea-be3b-6187a4a9f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold',-1)\n",
    "spark.conf.set('spark.sql.adaptive.enabled',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b926ea9d-5209-4742-8d26-a0a3335e8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [\n",
    "    (1,\"Alice\"),\n",
    "    (2,\"Bob\"),\n",
    "    (3,\"Charlie\"),\n",
    "    (4,\"David\"),\n",
    "    (5,\"Eva\")\n",
    "]\n",
    "df1 = spark.createDataFrame(data1, [\"id\",\"name\"])\n",
    "\n",
    "data2 = [\n",
    "    (1,50000),\n",
    "    (2,60000),\n",
    "    (3,70000),\n",
    "    (6,80000)\n",
    "]\n",
    "df2 = spark.createDataFrame(data2, [\"id\",\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dda383f-f6a5-4515-a76d-e8003ce5fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Join\n",
    "df_left_join = df1.join(df2,df1['id'] == df2['id'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5e65b7-7ce6-41cb-a000-7879b0581bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+\n",
      "| id|   name|  id|salary|\n",
      "+---+-------+----+------+\n",
      "|  5|    Eva|NULL|  NULL|\n",
      "|  1|  Alice|   1| 50000|\n",
      "|  3|Charlie|   3| 70000|\n",
      "|  2|    Bob|   2| 60000|\n",
      "|  4|  David|NULL|  NULL|\n",
      "+---+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6f820-ddde-4798-a3c8-68bf07654063",
   "metadata": {},
   "source": [
    "## Driver Memory Management\n",
    "\n",
    "- The Driver in Spark is the master process that coordinates all tasks.\n",
    "- It holds metadata, task scheduling information, DAGs and more.\n",
    "- Driver Memory -> Memory allocated to the Driver process when the spark job runs\n",
    "- If Driver runs out of memory -> job fails with OutOfMemoryError\n",
    "- The Driver memory is divided into two memory types\n",
    "1. **JVM Heap Memory**: This is the main memory allocated to the Java Virtual Machine where Spark’s core data structures live (e.g., RDDs, DataFrames, metadata). It is used for Task scheduling, Query planning and optimization, Caching metadata, broadcast variables, etc. JVM heap memory is where Spark runs its logic.\n",
    "2. **OverHead Memory**: This is extra memory reserved outside the JVM heap. It is used for Native memory (like PySpark or Pandas UDFs), Thread stacks, Internal buffers, Memory management by the OS. Overhead memory prevents out-of-memory errors during native or I/O-heavy operations. If you’re using PySpark or UDFs, increasing overhead memory is often necessary. This is max(10% of JVM Heap Memory, 384 MB)\n",
    "\n",
    "#### Driver Out of Memory\n",
    "When the size of the output from executors goes out of the range of the driver memory we get this error. We can mitigate this by avoiding heavy functions that return too much data like df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c3a461-21ee-4eb0-b161-462005572e42",
   "metadata": {},
   "source": [
    "## Executor Memory Management\n",
    "\n",
    "- The Executor Memory management is broadly divided into four categories:\n",
    "1. JVM Heap Memory: This memory is further broken down into three memories  \n",
    "   a. **Reserved Memory** [300 MB]: Is allocated 300 MB by default  \n",
    "   b. **User Memory** [0.4*(Total memory - Reserved Memory)]: This stores all the user defined functions.  \n",
    "   c. **Spark Memory Pool** [0.6*(Total memory - Reserved Memory)]:\n",
    "   - This memory stores all the cache and transformations that are required.\n",
    "   - 50% memory is used for caching and storing (Long Term Memory). Also called as **Storage Memory**\n",
    "   - 50% memory is used for transformations (Short Term Memory). Also called as **Executor Memory**\n",
    "   - However, this partition can be changed. This is called as allocation and borrowing\n",
    "   - Executor Memory can eliminate storage memory using LRU method but storage memory cannot eliminate the executor memory  \n",
    "3. Off-Heap Memory: Managed by the user and the default is zero\n",
    "4. Overhead Memory: This is extra memory outside the JVM heap memory -> max(10% of executor memory, 384 MB)\n",
    "5. Pyspark Momory: Used rarely and default is zero\n",
    "\n",
    "\n",
    "#### Executor Out of Memory\n",
    "This is caused because of one of the following:\n",
    "- **Large data per task (partition too big):** A single task processes more data than the executor can hold.\n",
    "- **Skewed data in joins or aggregations:** One key has too much data → some executors do all the work and crash.\n",
    "- **Improper caching/persisting:** Caching a large DataFrame without enough memory (especially with MEMORY_ONLY) leads to eviction or OOM.\n",
    "- **Use of wide transformations:** Operations like groupByKey, join, sort cause shuffle and large intermediate data in memory.\n",
    "- **Calling collect() on large DataFrames:** Tries to bring all data to the driver or executor memory → instant crash if it can't fit.\n",
    "- **Heavy PySpark UDFs or Pandas UDFs:** Native code or Python logic consumes off-heap memory → hits memoryOverhead limit.\n",
    "- **Too many tasks per executor:** Multiple tasks run concurrently, each using memory → total usage exceeds limit.\n",
    "- **Improper configuration:** --executor-memory or spark.executor.memoryOverhead is set too low.\n",
    "- **GC overhead without error:** JVM spends too much time in garbage collection due to memory pressure (near-OOM symptom).\n",
    "\n",
    "\n",
    "#### Salting\n",
    "\n",
    "Salting is a technique used in Spark to handle data skew, which occurs when one or a few keys in a join or aggregation operation have significantly more data than others. This imbalance causes Spark to assign a disproportionately large amount of data to a single task or executor, leading to performance bottlenecks or even out-of-memory errors. Salting mitigates this by artificially distributing skewed keys across multiple partitions. It works by appending a random \"salt\" value (like a number) to the skewed key, effectively transforming one heavy key into multiple lighter keys (e.g., \"India\" becomes \"India_1\", \"India_2\", etc.). The other dataset (in the case of a join) is also modified to match this salted structure. After performing the join or aggregation, the results can optionally be de-salted or recombined. This approach helps achieve better parallelism and load balancing during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1ef6f-e811-42db-9112-9b8fb3669db9",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78f7f383-e1d5-4984-8909-ad70ea27113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [\n",
    "    (1,\"Alice\"),\n",
    "    (2,\"Bob\"),\n",
    "    (3,\"Charlie\"),\n",
    "    (4,\"David\"),\n",
    "    (5,\"Eva\")\n",
    "]\n",
    "df1 = spark.createDataFrame(data1, [\"id\",\"name\"])\n",
    "df3 = spark.createDataFrame(data1, [\"id\",\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c776d44c-370c-4c61-9d03-951d9e0838f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "|  4|  David|\n",
      "|  5|    Eva|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a2220f2-7166-4a24-99ae-27cf9fd6000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('flag',lit('Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb84775-e97a-4b37-8c52-f2e377ee67bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+\n",
      "| id|   name|flag|\n",
      "+---+-------+----+\n",
      "|  1|  Alice| Yes|\n",
      "|  2|    Bob| Yes|\n",
      "|  3|Charlie| Yes|\n",
      "|  4|  David| Yes|\n",
      "|  5|    Eva| Yes|\n",
      "+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca7912-4555-4a8d-a29e-89f6e5ad080a",
   "metadata": {},
   "source": [
    "#### Without Caching\n",
    "\n",
    "Here we are just creating df2 using df1. What happens here is that once the dataframe df2 is called for creation the dataframe df1 is removed from the memory. Then how does df2 get created if df1 is not in memory? This is done by using the DAG for df1. However, since the df1 is created this is not efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d066ca10-28e8-4f49-9113-49f8c22d11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.filter(col('id') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "215c75d0-f095-4344-8a4f-897216698c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| id| name|flag|\n",
      "+---+-----+----+\n",
      "|  1|Alice| Yes|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adbc17d0-a23d-4813-9f1a-26dfd512d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#56L, name#57, Yes AS flag#73]\n",
      "+- *(1) Filter (isnotnull(id#56L) AND (id#56L = 1))\n",
      "   +- *(1) Scan ExistingRDD[id#56L,name#57]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00128191-fcfc-4cb6-b7db-0f147ff60ec5",
   "metadata": {},
   "source": [
    "#### With Caching\n",
    "\n",
    "In this we will recreate the dataframe df3 same as df1 but now we will cache the df3 and then create df4 using the same operations as that of df2. Then we will explain df4 and see the difference between the df2 and df4 creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca9ba309-4514-42cb-858d-a4734f644c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8431c415-2f25-4b20-afe0-aac29c5698e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.filter(col('id') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55ea83f6-8c55-434c-aca6-6924bd2e0ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "833b8693-4032-4391-adfb-6828525ee05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Filter (isnotnull(id#60L) AND (id#60L = 1))\n",
      "   +- InMemoryTableScan [id#60L, name#61], [isnotnull(id#60L), (id#60L = 1)]\n",
      "         +- InMemoryRelation [id#60L, name#61], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               +- *(1) Scan ExistingRDD[id#60L,name#61]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02928a-f215-4327-a8c9-1be5dc29fddc",
   "metadata": {},
   "source": [
    "## Storage Levels for Cache\n",
    "\n",
    "1. **MEMEORY_AND_DISK**\n",
    "- Tries to store the data in memory first.\n",
    "- If memory is not enough, spill the rest to the disk.\n",
    "- df.cache() = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "2. **MEMORY_ONLY**\n",
    "- Data is stored as RAM in deserialized Java objects.\n",
    "- If not enough memory, recompute the partitions when needed.\n",
    "\n",
    "3. **DISK_ONLY**\n",
    "- Stores data on the disk\n",
    "- Slowest option.\n",
    "\n",
    "4. **OFF_HEAP**\n",
    "- Uses off-heap memory(outside JVM heap)\n",
    "- Must be enabled with spark.memory.offHeap.enabled=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98d56b7e-00e8-44d5-b6b9-be5f60e9aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c06207-2cb8-448f-9dd6-b689cf066ced",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "Partitioning in Spark refers to how data is divided into smaller chunks (partitions) for parallel processing across the cluster. This can be done in memory using functions like .repartition() or .coalesce() to control the number and distribution of tasks across executors. It also applies to on-disk storage — when writing data with .write.partitionBy(column), Spark creates folder structures based on that column (e.g., /year=2023/). Proper partitioning improves parallelism and query performance by reducing data shuffling and task imbalance.\n",
    "\n",
    "### Pruning\n",
    "Pruning, specifically column pruning, is a logical optimization where Spark reads only the columns requested in a query instead of loading the entire dataset. This is especially effective with columnar storage formats like Parquet and ORC. For example, when you run df.select(\"name\"), Spark will only load the name column from disk, skipping the rest. This reduces I/O and memory usage, speeding up query execution.\n",
    "\n",
    "### Partition Pruning\n",
    "Partition pruning is a physical read-time optimization where Spark avoids scanning irrelevant data partitions on disk based on query filters. If a dataset is partitioned by country and your query includes filter(country = 'US'), Spark will only scan the /country=US/ folder rather than reading all partitions. This significantly improves performance. Partition pruning can be static (filter known at compile time) or dynamic (filter resolved at runtime, often in joins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d71221e-0aa6-422a-af00-50e7caa1cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    (\"Alice\",'HR',1000),\n",
    "    (\"Bob\",'IT',2000),\n",
    "    (\"Charlie\",'HR',1500),\n",
    "    (\"David\",'Finance',2500),\n",
    "    (\"Eve\",'IT',1800),\n",
    "    (\"Frank\",'Finance',2200)\n",
    "]\n",
    "\n",
    "columns = ['name','department','salary']\n",
    "\n",
    "df = spark.createDataFrame(sample_data, columns)\n",
    "\n",
    "output_path = './Output/'\n",
    "output_path_new = './OutputNew/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb853700-a271-4ef4-b6b6-8df44d50357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\\\n",
    "    .mode('overwrite')\\\n",
    "    .partitionBy('department')\\\n",
    "    .parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e49f3a3-43e7-46d0-a65a-80f5c3cbf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\\\n",
    "    .mode('overwrite')\\\n",
    "    .parquet(output_path_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7bd260-763c-499c-9cbf-4b735277e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_part = spark.read.format('parquet')\\\n",
    "                            .load(output_path)\\\n",
    "                            .filter(col('department') == 'HR')                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3064a1b-83ea-406c-b250-7fa7c677bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|   name|salary|department|\n",
      "+-------+------+----------+\n",
      "|Charlie|  1500|        HR|\n",
      "|  Alice|  1000|        HR|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_part.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d40f4aea-bbf5-453b-8c40-055c19f02659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_part = spark.read.format('parquet')\\\n",
    "                            .load(output_path_new)\\\n",
    "                            .filter(col('department') == 'HR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "423bc648-3e6a-41ae-9596-ea60875fa892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   name|department|salary|\n",
      "+-------+----------+------+\n",
      "|Charlie|        HR|  1500|\n",
      "|  Alice|        HR|  1000|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_without_part.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b0bfb-de2a-45fb-b98c-a9554d3ce70f",
   "metadata": {},
   "source": [
    "### Impact of Partition Pruning.\n",
    "\n",
    "When we perform without partition and try to filter the rows the output is\n",
    "- Scan parquet  \n",
    "**number of files read: 7**  \n",
    "**scan time total (min, med, max ) -> 127 ms (15 ms, 16 ms, 26 ms )**  \n",
    "metadata time: 0 ms  \n",
    "**size of files read: 6.2 KiB**  \n",
    "number of output rows: 2\n",
    "\n",
    "While when we perform parititioning we get the output as\n",
    "- Scan parquet  \n",
    "**number of files read: 2**  \n",
    "**scan time total (min, med, max ) -> 31 ms (15 ms, 16 ms, 16 ms )**  \n",
    "dynamic partition pruning time: 0 ms  \n",
    "metadata time: 13 ms  \n",
    "**size of files read: 1476.0 B**  \n",
    "number of output rows: 2  \n",
    "number of partitions read: 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29173228-6575-4779-8d23-a032ca31cc54",
   "metadata": {},
   "source": [
    "### Dynamic Partition Pruning\n",
    "\n",
    "Dynamic Partition Pruning (DPP) is a performance optimization in Spark SQL that reduces the amount of data read during a join by dynamically filtering partitions at runtime instead of reading all partitions of a table.\n",
    "\n",
    "🔹 How It Works:\n",
    "When you run a query that joins a large partitioned table with another table, Spark doesn't always know in advance (at compile time) which partitions it needs. With Dynamic Partition Pruning, Spark waits until the broadcast or shuffle of the smaller table completes, extracts the relevant partition values, and then prunes the partitions of the larger table at execution time.\n",
    "\n",
    "🔹 Example:\n",
    "\n",
    "SELECT *\n",
    "FROM large_sales s\n",
    "JOIN region_filter r\n",
    "ON s.region_id = r.region_id\n",
    "\n",
    "If large_sales is partitioned by region_id, DPP ensures Spark only scans the matching partitions based on values from region_filter — instead of scanning all region_id partitions.\n",
    "\n",
    "🔹 Why It Matters:\n",
    "- Improves performance (reduces I/O and scan time)\n",
    "- Especially useful when filter values are only known at runtime (e.g., in subqueries or joins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
