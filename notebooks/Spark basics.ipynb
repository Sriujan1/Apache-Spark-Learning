{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8501a514-4f11-4043-92bd-c428b4930c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae30a32d-599e-4ac7-92bd-b0c53d3f7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark_env\n",
    "\n",
    "spark = spark_env.create_spark_session('practice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0140825-7d24-4543-91e8-88142865c278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://harihar.cable.rcn.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11cc9e65f30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d77e01-9791-4b45-8b8a-356937e26b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [\n",
    "    ('John',24,'Chicago'),\n",
    "    ('Jack',35,'New York'),\n",
    "    ('Doe',28,'San Francisco'),\n",
    "    ('Kim',32,'Chicago'),\n",
    "    ('Jerry',42,'Phoenix')\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('age',IntegerType(),True),\n",
    "    StructField('city',StringType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(df,schema)\n",
    "\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41467ebf-5fab-4d31-9ed3-b10ec50280dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.filter(col('age') > 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed1ed46-e931-422e-a621-a9f46d76235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+\n",
      "| name|age|         city|\n",
      "+-----+---+-------------+\n",
      "| Jack| 35|     New York|\n",
      "|  Doe| 28|San Francisco|\n",
      "|  Kim| 32|      Chicago|\n",
      "|Jerry| 42|      Phoenix|\n",
      "+-----+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b948797-1f10-4d91-a692-bed7166dff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Filter (2)\n",
      "+- * Scan ExistingRDD (1)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [3]: [name#0, age#1, city#2]\n",
      "Arguments: [name#0, age#1, city#2], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [3]: [name#0, age#1, city#2]\n",
      "Condition : (isnotnull(age#1) AND (age#1 > 26))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.explain('formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1743252a-3d82-4e2c-b243-4c322cc452b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_transformation_df = df.filter(col('age') > 20)\\\n",
    "                                .select(col('name'),col('city'))\\\n",
    "                                .orderBy(col('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b30c9b-efc0-4fa5-ae86-9c613611b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "| name|         city|\n",
      "+-----+-------------+\n",
      "|  Kim|      Chicago|\n",
      "| John|      Chicago|\n",
      "| Jack|     New York|\n",
      "|Jerry|      Phoenix|\n",
      "|  Doe|San Francisco|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiple_transformation_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2daccd33-501d-49b5-8f5d-ebb4bd558d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (6)\n",
      "+- Sort (5)\n",
      "   +- Exchange (4)\n",
      "      +- Project (3)\n",
      "         +- Filter (2)\n",
      "            +- Scan ExistingRDD (1)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD\n",
      "Output [3]: [name#0, age#1, city#2]\n",
      "Arguments: [name#0, age#1, city#2], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [name#0, age#1, city#2]\n",
      "Condition : (isnotnull(age#1) AND (age#1 > 20))\n",
      "\n",
      "(3) Project\n",
      "Output [2]: [name#0, city#2]\n",
      "Input [3]: [name#0, age#1, city#2]\n",
      "\n",
      "(4) Exchange\n",
      "Input [2]: [name#0, city#2]\n",
      "Arguments: rangepartitioning(city#2 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=56]\n",
      "\n",
      "(5) Sort\n",
      "Input [2]: [name#0, city#2]\n",
      "Arguments: [city#2 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(6) AdaptiveSparkPlan\n",
      "Output [2]: [name#0, city#2]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiple_transformation_df.explain('formatted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790fe32-646e-4fc5-9627-46e98104cd98",
   "metadata": {},
   "source": [
    "### Must know Actions in Spark\n",
    "---\n",
    "- **collect()** ‚Äì Retrieves the entire RDD/DataFrame to the driver. Be cautious: can cause OOM if data is too large.\n",
    "- **count()** ‚Äì Returns the number of elements in the dataset.\n",
    "- **take(n)** ‚Äì Returns the first n elements as an array.\n",
    "- **first()** ‚Äì Returns the first element (like take(1)[0]).\n",
    "- **show()** (DataFrame) ‚Äì Displays the top rows in a tabular format.\n",
    "- **reduce(func)** ‚Äì Aggregates the dataset using a function (e.g., sum, max).\n",
    "- **foreach(func)** ‚Äì Applies a function to each element (executed on the worker nodes).\n",
    "- **saveAsTextFile(path)** ‚Äì Saves the RDD as a text file to the specified path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597c9c8-45d5-427f-9a34-d125fcf5f67c",
   "metadata": {},
   "source": [
    "### Good to Know Actions in Spark\n",
    "---\n",
    "- **collectAsMap()** ‚Äì For (key, value) RDDs; collects to a map at the driver.\n",
    "- **countByValue()** ‚Äì Returns the count of each unique value.\n",
    "- **takeSample(withReplacement, num)** ‚Äì Samples num elements from the dataset.\n",
    "- **top(n)** ‚Äì Returns the top n elements (requires ordering).\n",
    "- **takeOrdered(n)** ‚Äì Returns the first n elements in sorted order.\n",
    "- **saveAsSequenceFile() / saveAsObjectFile()** ‚Äì Specialized saving methods for RDDs.\n",
    "- **foreachPartition()** ‚Äì Operates on each partition, useful for batch inserts or connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e0392-24aa-4a52-9085-9f9a09f16c4a",
   "metadata": {},
   "source": [
    "## üîÅ Spark Action Function Best Practices\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 1. Understanding difference between `foreach()` and `foreachPartition()`\n",
    "\n",
    "| Function             | Behavior                                                                 |\n",
    "|----------------------|--------------------------------------------------------------------------|\n",
    "| `foreach()`          | Applies a function to **each element** ‚Äî one record at a time.           |\n",
    "| `foreachPartition()` | Applies a function to **each partition** ‚Äî one partition at a time.      |\n",
    "\n",
    "**Why it matters:**  \n",
    "- `foreachPartition()` is **more efficient** for I/O-bound tasks like writing to a database or calling external APIs.\n",
    "- Instead of opening a DB connection for every row (which `foreach()` might do), you can open **one connection per partition**.\n",
    "\n",
    "**‚úÖ Use `foreachPartition()` when:**\n",
    "- Writing to databases\n",
    "- Performing bulk writes\n",
    "- Making batched API calls\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. Knowing when to use `collect()` vs. `take()`\n",
    "\n",
    "| Function      | Behavior                                | Risk              |\n",
    "|---------------|-----------------------------------------|-------------------|\n",
    "| `collect()`   | Returns **all rows** to the driver       | OOM if too large  |\n",
    "| `take(n)`     | Returns **only first n rows**           | Safe and fast     |\n",
    "\n",
    "**Why it matters:**  \n",
    "- `collect()` can crash your driver if your data is large.\n",
    "- Use `take()` to **inspect samples** or debug pipelines without pulling full datasets.\n",
    "\n",
    "**‚úÖ Best practice:**\n",
    "- Use `.take(5)` or `.limit(5).collect()` for small previews.\n",
    "- Use `.collect()` **only** when you're **sure** the result fits in memory.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 3. Using actions to trigger lazy evaluation properly\n",
    "\n",
    "- Spark uses **lazy evaluation** ‚Äî transformations like `map()`, `filter()`, `select()` do **nothing** until an **action** is called.\n",
    "- Actions include: `show()`, `count()`, `collect()`, `save()`, `foreach()`.\n",
    "\n",
    "**Why it matters:**  \n",
    "- Lazy evaluation allows Spark to **optimize execution** by building a logical plan.\n",
    "- This is what makes Spark powerful and efficient.\n",
    "\n",
    "**‚úÖ Remember:**\n",
    "Transformations define the logic ‚Üí **Actions trigger execution**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 4. Combining `reduce()` or `aggregate()` for custom distributed aggregations\n",
    "\n",
    "| Function      | Purpose                                   | Customization       |\n",
    "|---------------|-------------------------------------------|---------------------|\n",
    "| `reduce()`    | Aggregates using an associative function (e.g., sum, max) | ‚ùå No zero value or type change |\n",
    "| `aggregate()` | Aggregates using two functions: one for within and one across partitions | ‚úÖ Flexible and safe |\n",
    "\n",
    "**Why it matters:**  \n",
    "- `reduce()` is simple but can‚Äôt handle empty RDDs or different result types.\n",
    "- `aggregate()` lets you:\n",
    "  - Define **initial zero value**.\n",
    "  - Use **different logic** for combining values in partition vs across partitions.\n",
    "\n",
    "**‚úÖ Use `aggregate()` when:**\n",
    "- You need to return a different type (e.g., average = sum/count).\n",
    "- You need robust, custom aggregation logic across partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2188fc3-e5d8-4289-8e46-77702fcdf92d",
   "metadata": {},
   "source": [
    "## Transformations in Spark\n",
    "\n",
    "**Narrow Transformation:** A transformation where each partition of the parent RDD is used by only one partition of the child RDD. No data shuffle.\n",
    "\n",
    "**Wide Transformation:** A transformation where data from multiple partitions may be required to compute a single partition of the child RDD. Involves shuffle.\n",
    "\n",
    "#### Examples of Transformations\n",
    "- **map()**\n",
    "- **filter()**\n",
    "- **select()**\n",
    "- **withColumn()**\n",
    "- **groupBy()**\n",
    "- **join()**\n",
    "---\n",
    "- Spark builds a lineage (DAG) of transformations and only evaluates it when an action is called.\n",
    "- Skilled Spark engineers optimize performance by minimizing wide transformations and strategically placing actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e597919-7fb8-4d1a-8e24-2fdbf616feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Wide Transformation\n",
    "\n",
    "wide_transformation_df = df.groupby('city').agg(max('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "543a683a-7aae-497e-9cc9-ad9961bcb611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|         city|max(age)|\n",
      "+-------------+--------+\n",
      "|      Chicago|      32|\n",
      "|     New York|      35|\n",
      "|San Francisco|      28|\n",
      "|      Phoenix|      42|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wide_transformation_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "519d8ec8-a3f4-48e7-a645-8e46602cf2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, max('age) AS max(age)#34]\n",
      "+- LogicalRDD [name#0, age#1, city#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, max(age): int\n",
      "Aggregate [city#2], [city#2, max(age#1) AS max(age)#34]\n",
      "+- LogicalRDD [name#0, age#1, city#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#2], [city#2, max(age#1) AS max(age)#34]\n",
      "+- Project [age#1, city#2]\n",
      "   +- LogicalRDD [name#0, age#1, city#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#2], functions=[max(age#1)], output=[city#2, max(age)#34])\n",
      "   +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=115]\n",
      "      +- HashAggregate(keys=[city#2], functions=[partial_max(age#1)], output=[city#2, max#44])\n",
      "         +- Project [age#1, city#2]\n",
      "            +- Scan ExistingRDD[name#0,age#1,city#2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wide_transformation_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29979a44-c981-4032-8608-93259101dd2d",
   "metadata": {},
   "source": [
    "### Repartition vs Coalesce\n",
    "\n",
    "**Coalesce:**\n",
    "\n",
    "coalesce() is a narrow transformation used to reduce the number of partitions by merging adjacent partitions, without triggering a full shuffle (or with minimal shuffling if necessary).\n",
    "coalesce() is much faster than repartition() when you're only reducing partitions, especially for writing to disk ‚Äî it helps avoid the problem of creating too many small files, making it a go-to for final write stages.\n",
    "\n",
    "**Repartition:**\n",
    "\n",
    "repartition() is a Spark transformation used to increase or reshuffle the number of partitions by performing a full shuffle of the data across the cluster.\n",
    "Unlike coalesce(), repartition() balances data evenly across all partitions, making it ideal before joins, caching, or expensive operations where load balancing improves performance ‚Äî though it‚Äôs more costly due to the shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b979eb-1211-4a05-89c0-a4cc84c66d1e",
   "metadata": {},
   "source": [
    "| Scenario                         | Partition Source                  | Number of Partitions |\n",
    "|----------------------------------|-----------------------------------|-----------------------|\n",
    "| Creating DataFrame from a Python list (`createDataFrame`) | `sparkContext.defaultParallelism` | Usually 12            |\n",
    "| After wide transformation (`groupBy`, `join`, etc.)        | `spark.sql.shuffle.partitions`    | Defaults to 200       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a389cfd3-cd84-47b0-b93a-f90028af7c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149cae50-67e1-4c0c-b5c8-132f9a46bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd491d49-7488-4854-a7dd-5aa83a9a6076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(3), REPARTITION_BY_NUM, [plan_id=127]\n",
      "   +- Scan ExistingRDD[name#0,age#1,city#2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.repartition(3)\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc3aee60-03d2-4134-88b5-c951c820d8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e77dcd06-5aec-4506-8f15-6feb0cd1b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Coalesce 1\n",
      "   +- Exchange RoundRobinPartitioning(3), REPARTITION_BY_NUM, [plan_id=146]\n",
      "      +- Scan ExistingRDD[name#0,age#1,city#2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.coalesce(1)\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a063b0cc-8679-4b73-97df-27ba2f90e77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a9bd99b-5b35-4d03-92ec-ee689f81425e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+\n",
      "| name|age|         city|\n",
      "+-----+---+-------------+\n",
      "|  Kim| 32|      Chicago|\n",
      "|Jerry| 42|      Phoenix|\n",
      "| John| 24|      Chicago|\n",
      "| Jack| 35|     New York|\n",
      "|  Doe| 28|San Francisco|\n",
      "+-----+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
